{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1hkhc10wNrGt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x4giRzM7NtHJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "cwJrQFQgN_BE",
        "outputId": "854f0b55-e330-4806-cc32-79643e6bd721"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                      Ok lar... Joking wif u oni...\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"spamdata_v2.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "fzPPOrVQWiW5",
        "outputId": "e8555c2b-a50d-4809-833f-adf3ac349a1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "676DPU1BOPdp",
        "outputId": "075808af-7b2e-4f0d-e888-e06e2e8abbf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    0.865937\n",
              "1    0.134063\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mfhSPF5jOWb7"
      },
      "outputs": [],
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "983fea7c2dc74dfaba7aa60147af85d1",
            "ccf5f7e5cc10493ca9c44b14fdec31dc",
            "59bae99ad63d4a3a8b8d622d95f7ad07",
            "689e66a8dff249449b5f0f5bbfffa037",
            "49dd79a9a65044ba8345deb250ce4b24",
            "47862cd626cf46619a5cc505fde02276",
            "cb5f7a2a5bcb47649703cc633f2fb685",
            "6d2355752eb74f348596a380d2347b73",
            "0e580433bec2453da54c0ce9ee027401",
            "bdfd7634b8bf42aa8794ada8d9e47173",
            "73fc7587f1bb49df8a0fc87ecfac7f3c",
            "4da1c15300b2468ab1a7e2df800ed39b",
            "645d520e8a1c4f1fa202c6c68c5ce6af",
            "3bb6b624b4ce4be788c38cb8d1936177",
            "ed9f97f9d12a49aa939b7595ad3cb27c",
            "88214abee8b9462f86369072d858ae9f",
            "b4bef5a685954e238b52c43eefe4c9e5",
            "94264f36ceb64d3881fed952bf579072",
            "8cf06dad410440f78c50e3527e858905",
            "edf0e4c1ae214a66abb717b20e3bffad",
            "4d56a47453914de3be15d7a515e5b210",
            "6831c2b733d74b31a41cb6eb971a25d7",
            "58cd585c531444a5b8613c2d85bab022",
            "b6423c858927455e8cbc5a953273466a"
          ]
        },
        "colab_type": "code",
        "id": "S1kY3gZjO2RE",
        "outputId": "4194574c-05d6-4d1d-c4c0-8dd89913ff79"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "983fea7c2dc74dfaba7aa60147af85d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e580433bec2453da54c0ce9ee027401",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4bef5a685954e238b52c43eefe4c9e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_zOKeOMeO-DT"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "oAH73n39PHLw",
        "outputId": "17b76300-71f2-464c-90b0-a5907f4f675a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# output\n",
        "print(sent_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "yKwbpeN_PMiu",
        "outputId": "9f843240-6cf4-46c9-80b7-dc9ab4e03602"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efee3369828>"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OXcswEIRPvGe"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tk5S7DWaP2t6"
      },
      "outputs": [],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QR-lXwmzQPd6"
      },
      "outputs": [],
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qUy9JKFYQYLp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wHZ0MC00RQA_"
      },
      "outputs": [],
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b3iEtGyYRd0A"
      },
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cBAJJVuJRliv"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "taXS0IilRn9J"
      },
      "outputs": [],
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "izY5xH5eR7Ur",
        "outputId": "4682d190-bf40-4824-89af-91983ae6b174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.57743559 3.72848948]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r1WvfY2vSGKi"
      },
      "outputs": [],
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rskLk8R_SahS"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yGXovFDlSxB5"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "k1USGTntS3TS",
        "outputId": "6c03e17f-476c-4741-eae5-c5722cb5d413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.656\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.345\n",
            "Validation Loss: 0.231\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.344\n",
            "Validation Loss: 0.194\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.223\n",
            "Validation Loss: 0.171\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.219\n",
            "Validation Loss: 0.178\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.215\n",
            "Validation Loss: 0.180\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.247\n",
            "Validation Loss: 0.262\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.224\n",
            "Validation Loss: 0.217\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.217\n",
            "Validation Loss: 0.148\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    122.\n",
            "  Batch   100  of    122.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.231\n",
            "Validation Loss: 0.639\n"
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "OacxUyizS8d1",
        "outputId": "c8b951c2-1f74-4a13-db65-8acd077995e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NZl0SZmFTRQA"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "colab_type": "code",
        "id": "Ms1ObHZxTYSI",
        "outputId": "47d01595-e519-4a58-8f2e-75596ea1512d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98       724\n",
            "           1       0.88      0.92      0.90       112\n",
            "\n",
            "    accuracy                           0.97       836\n",
            "   macro avg       0.93      0.95      0.94       836\n",
            "weighted avg       0.97      0.97      0.97       836\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "colab_type": "code",
        "id": "YqzLS7rHTp4T",
        "outputId": "d3abc432-5ad0-41e5-cfc2-d1d1192f5672"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>710</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    0    1\n",
              "row_0          \n",
              "0      710   14\n",
              "1        9  103"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jpX1uTwjUPY6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "class SentenceEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Sentence encoder module that encodes a sentence containing entities into a feature vector.\n",
        "    It uses BERT as embedding layer and CNN as encoding layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_bert=\"bert-base-uncased\", hidden_size=768, max_length=64):\n",
        "        super(SentenceEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained(pretrained_bert)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert)\n",
        "        \n",
        "        # CNN layer\n",
        "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: List of sentences with marked entity positions\n",
        "        \n",
        "        Returns:\n",
        "            Sentence embeddings\n",
        "        \"\"\"\n",
        "        # Process input sentences with BERT\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            inputs,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        outputs = self.bert(\n",
        "            input_ids=encoded_inputs[\"input_ids\"].to(self.bert.device),\n",
        "            attention_mask=encoded_inputs[\"attention_mask\"].to(self.bert.device),\n",
        "            token_type_ids=encoded_inputs[\"token_type_ids\"].to(self.bert.device)\n",
        "        )\n",
        "        \n",
        "        # Get the word embeddings from BERT\n",
        "        word_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "        \n",
        "        # Apply CNN\n",
        "        word_embeddings = word_embeddings.transpose(1, 2)  # [batch_size, hidden_size, seq_len]\n",
        "        conv_output = self.conv(word_embeddings)  # [batch_size, hidden_size, seq_len]\n",
        "        \n",
        "        # Max pooling\n",
        "        sentence_embeddings = F.max_pool1d(conv_output, kernel_size=conv_output.size(2))  # [batch_size, hidden_size, 1]\n",
        "        sentence_embeddings = sentence_embeddings.squeeze(2)  # [batch_size, hidden_size]\n",
        "        \n",
        "        return sentence_embeddings\n",
        "\n",
        "class ContextAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Context attention mechanism to highlight crucial instances in support set.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size=768, num_heads=12):\n",
        "        super(ContextAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        \n",
        "        assert self.head_dim * num_heads == hidden_size, \"hidden_size must be divisible by num_heads\"\n",
        "    \n",
        "    def forward(self, support_set):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            support_set: Support set embeddings [N, K, hidden_size]\n",
        "            \n",
        "        Returns:\n",
        "            Weighted support set embeddings [N, K, hidden_size]\n",
        "        \"\"\"\n",
        "        N, K, D = support_set.size()  # N relations, K instances per relation, D hidden size\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        support_set = support_set.view(N, K, self.num_heads, self.head_dim)\n",
        "        support_set = support_set.permute(0, 2, 1, 3)  # [N, num_heads, K, head_dim]\n",
        "        \n",
        "        # Calculate scaled dot-product attention\n",
        "        attention_scores = torch.matmul(support_set, support_set.transpose(-1, -2))  # [N, num_heads, K, K]\n",
        "        attention_scores = attention_scores / (self.head_dim ** 0.5)\n",
        "        \n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # [N, num_heads, K, K]\n",
        "        \n",
        "        # Apply attention weights to support set\n",
        "        weighted_support = torch.matmul(attention_weights, support_set)  # [N, num_heads, K, head_dim]\n",
        "        \n",
        "        # Reshape back\n",
        "        weighted_support = weighted_support.permute(0, 2, 1, 3)  # [N, K, num_heads, head_dim]\n",
        "        weighted_support = weighted_support.reshape(N, K, D)  # [N, K, hidden_size]\n",
        "        \n",
        "        return weighted_support\n",
        "\n",
        "class ProtoCABTwithBERT(nn.Module):\n",
        "    \"\"\"\n",
        "    Context Attention-based Prototypical Networks with BERT for few-shot relation classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_bert=\"bert-base-uncased\", hidden_size=768, num_heads=12):\n",
        "        super(ProtoCABTwithBERT, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Sentence encoder\n",
        "        self.encoder = SentenceEncoder(pretrained_bert, hidden_size)\n",
        "        \n",
        "        # Context attention\n",
        "        self.context_attention = ContextAttention(hidden_size, num_heads)\n",
        "    \n",
        "    def forward(self, support, query):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            support: Support set, N relations with K instances each\n",
        "                     [N, K] list of sentences\n",
        "            query: Query instance, a sentence\n",
        "            \n",
        "        Returns:\n",
        "            Relation probabilities for the query\n",
        "        \"\"\"\n",
        "        N = len(support)  # Number of relations\n",
        "        K = len(support[0])  # Number of instances per relation\n",
        "        \n",
        "        # Flatten support set for batch processing\n",
        "        support_flat = [instance for relation in support for instance in relation]\n",
        "        \n",
        "        # Encode support set instances\n",
        "        support_embeddings = self.encoder(support_flat)\n",
        "        support_embeddings = support_embeddings.view(N, K, -1)  # [N, K, hidden_size]\n",
        "        \n",
        "        # Apply context attention to get weighted support embeddings\n",
        "        support_embeddings = self.context_attention(support_embeddings)  # [N, K, hidden_size]\n",
        "        \n",
        "        # Calculate prototypes for each relation\n",
        "        prototypes = torch.mean(support_embeddings, dim=1)  # [N, hidden_size]\n",
        "        \n",
        "        # Encode query instance\n",
        "        query_embedding = self.encoder([query])  # [1, hidden_size]\n",
        "        \n",
        "        # Calculate distances between query and prototypes\n",
        "        distances = torch.sum((query_embedding.unsqueeze(1) - prototypes.unsqueeze(0)) ** 2, dim=2)  # [1, N]\n",
        "        distances = distances.squeeze(0)  # [N]\n",
        "        \n",
        "        # Calculate probabilities using softmax with negative distances\n",
        "        logits = -distances\n",
        "        probabilities = F.softmax(logits, dim=0)\n",
        "        \n",
        "        return probabilities\n",
        "\n",
        "# Training function\n",
        "def train(model, train_data, val_data, epochs=10000, lr=2e-5, n_way=5, k_shot=5):\n",
        "    \"\"\"\n",
        "    Train the model for few-shot relation classification.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_data: Training data organized by relations\n",
        "        val_data: Validation data organized by relations\n",
        "        epochs: Number of training iterations\n",
        "        lr: Learning rate\n",
        "        n_way: Number of relations in each episode\n",
        "        k_shot: Number of instances per relation in support set\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Sample classes for this episode\n",
        "        sampled_relations = np.random.choice(len(train_data), n_way, replace=False)\n",
        "        \n",
        "        # Prepare support set and query set\n",
        "        support = []\n",
        "        query_examples = []\n",
        "        query_labels = []\n",
        "        \n",
        "        for i, rel_id in enumerate(sampled_relations):\n",
        "            instances = train_data[rel_id]\n",
        "            # Randomly select k_shot instances for support set\n",
        "            support_indices = np.random.choice(len(instances), k_shot, replace=False)\n",
        "            support.append([instances[idx] for idx in support_indices])\n",
        "            \n",
        "            # Select one instance for query\n",
        "            query_idx = np.random.choice(len(instances))\n",
        "            while query_idx in support_indices:\n",
        "                query_idx = np.random.choice(len(instances))\n",
        "            \n",
        "            query_examples.append(instances[query_idx])\n",
        "            query_labels.append(i)\n",
        "        \n",
        "        # Create target tensor\n",
        "        targets = torch.tensor(query_labels, dtype=torch.long)\n",
        "        \n",
        "        # Forward pass for each query\n",
        "        logits = []\n",
        "        for query in query_examples:\n",
        "            prob = model(support, query)\n",
        "            logits.append(prob.unsqueeze(0))\n",
        "        \n",
        "        logits = torch.cat(logits, dim=0)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Validation every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            val_acc = evaluate(model, val_data, n_way, k_shot)\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "            \n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                print(f\"New best accuracy: {best_acc:.4f}\")\n",
        "                # Save the model\n",
        "                torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_data, n_way=5, k_shot=5, num_episodes=600):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation data.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        val_data: Validation data organized by relations\n",
        "        n_way: Number of relations in each episode\n",
        "        k_shot: Number of instances per relation in support set\n",
        "        num_episodes: Number of evaluation episodes\n",
        "    \n",
        "    Returns:\n",
        "        Average accuracy across episodes\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_episodes):\n",
        "            # Sample classes for this episode\n",
        "            sampled_relations = np.random.choice(len(val_data), n_way, replace=False)\n",
        "            \n",
        "            # Prepare support set and query set\n",
        "            support = []\n",
        "            query_examples = []\n",
        "            query_labels = []\n",
        "            \n",
        "            for i, rel_id in enumerate(sampled_relations):\n",
        "                instances = val_data[rel_id]\n",
        "                # Randomly select k_shot instances for support set\n",
        "                support_indices = np.random.choice(len(instances), k_shot, replace=False)\n",
        "                support.append([instances[idx] for idx in support_indices])\n",
        "                \n",
        "                # Select one instance for query\n",
        "                query_idx = np.random.choice(len(instances))\n",
        "                while query_idx in support_indices:\n",
        "                    query_idx = np.random.choice(len(instances))\n",
        "                \n",
        "                query_examples.append(instances[query_idx])\n",
        "                query_labels.append(i)\n",
        "            \n",
        "            # Predict for each query\n",
        "            for i, query in enumerate(query_examples):\n",
        "                prob = model(support, query)\n",
        "                predicted_relation = torch.argmax(prob).item()\n",
        "                \n",
        "                if predicted_relation == query_labels[i]:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "# Main function to run the code\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load data\n",
        "    # This is a placeholder - you would load your actual FewRel dataset here\n",
        "    # train_data = load_fewrel_data(\"train\")\n",
        "    # val_data = load_fewrel_data(\"val\")\n",
        "    \n",
        "    # Initialize model\n",
        "    model = ProtoCABTwithBERT().to(device)\n",
        "    \n",
        "    # Set hyperparameters\n",
        "    n_way = 5\n",
        "    k_shot = 5\n",
        "    lr = 2e-5\n",
        "    epochs = 10000\n",
        "    \n",
        "    # Train model\n",
        "    # trained_model = train(model, train_data, val_data, epochs, lr, n_way, k_shot)\n",
        "    \n",
        "    # Evaluate final model\n",
        "    # final_acc = evaluate(trained_model, val_data, n_way, k_shot)\n",
        "    # print(f\"Final validation accuracy: {final_acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class Proto_CATT_BERT(nn.Module):\n",
        "    \"\"\"\n",
        "    Context Attention-based Prototypical Networks with BERT for Few-Shot Relation Classification\n",
        "    \"\"\"\n",
        "    def __init__(self, bert_model='bert-base-uncased', hidden_size=768, conv_window_size=3, num_heads=12):\n",
        "        super(Proto_CATT_BERT, self).__init__()\n",
        "        \n",
        "        # BERT embedding layer\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # CNN encoding layer\n",
        "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=conv_window_size, \n",
        "                             padding=(conv_window_size-1)//2)\n",
        "        \n",
        "        # Number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        \n",
        "        assert self.head_dim * num_heads == hidden_size, \"hidden_size must be divisible by num_heads\"\n",
        "    \n",
        "    def sentence_encoder(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        \"\"\"\n",
        "        Encode a sentence using BERT and CNN\n",
        "        \"\"\"\n",
        "        # Get BERT embeddings\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        word_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
        "        \n",
        "        # Apply CNN for feature extraction\n",
        "        # Transpose for conv1d which expects (batch_size, channels, seq_len)\n",
        "        word_embeddings = word_embeddings.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
        "        conv_output = self.conv(word_embeddings)  # (batch_size, hidden_size, seq_len)\n",
        "        \n",
        "        # Apply max pooling over sequence length\n",
        "        sentence_embedding = F.max_pool1d(conv_output, kernel_size=conv_output.size(2))\n",
        "        sentence_embedding = sentence_embedding.squeeze(2)  # (batch_size, hidden_size)\n",
        "        \n",
        "        return sentence_embedding\n",
        "    \n",
        "    def context_attention(self, support_set):\n",
        "        \"\"\"\n",
        "        Apply context attention to the support set\n",
        "        \n",
        "        Implementation of equation: softmax(S*S^T/sqrt(d_w))*S with multi-head attention\n",
        "        \"\"\"\n",
        "        batch_size, num_instances, _ = support_set.size()\n",
        "        \n",
        "        # Split into multiple heads\n",
        "        support_set_heads = support_set.view(batch_size, num_instances, self.num_heads, self.head_dim)\n",
        "        support_set_heads = support_set_heads.permute(0, 2, 1, 3)  # (batch_size, num_heads, num_instances, head_dim)\n",
        "        \n",
        "        # Calculate attention for each head\n",
        "        attention_scores = torch.matmul(support_set_heads, support_set_heads.transpose(-2, -1))\n",
        "        attention_scores = attention_scores / (self.head_dim ** 0.5)  # Scale by sqrt(d_w)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Apply attention weights\n",
        "        weighted_heads = torch.matmul(attention_weights, support_set_heads)\n",
        "        \n",
        "        # Combine heads\n",
        "        weighted_heads = weighted_heads.permute(0, 2, 1, 3)  # (batch_size, num_instances, num_heads, head_dim)\n",
        "        weighted_support = weighted_heads.reshape(batch_size, num_instances, self.hidden_size)\n",
        "        \n",
        "        return weighted_support\n",
        "    \n",
        "    def forward(self, query_inputs, support_inputs, n_way, k_shot):\n",
        "        \"\"\"\n",
        "        Forward pass for the model\n",
        "        \"\"\"\n",
        "        # Process query instances\n",
        "        query_embedding = self.sentence_encoder(\n",
        "            query_inputs['input_ids'], \n",
        "            attention_mask=query_inputs.get('attention_mask', None), \n",
        "            token_type_ids=query_inputs.get('token_type_ids', None)\n",
        "        )  # (batch_size, hidden_size)\n",
        "        \n",
        "        # Process support set instances\n",
        "        batch_size = support_inputs['input_ids'].size(0)\n",
        "        support_instance_count = support_inputs['input_ids'].size(1)  # n_way * k_shot\n",
        "        \n",
        "        # Reshape for processing each instance individually\n",
        "        flat_input_ids = support_inputs['input_ids'].view(-1, support_inputs['input_ids'].size(-1))\n",
        "        flat_attention_mask = None\n",
        "        if 'attention_mask' in support_inputs:\n",
        "            flat_attention_mask = support_inputs['attention_mask'].view(-1, support_inputs['attention_mask'].size(-1))\n",
        "        flat_token_type_ids = None\n",
        "        if 'token_type_ids' in support_inputs:\n",
        "            flat_token_type_ids = support_inputs['token_type_ids'].view(-1, support_inputs['token_type_ids'].size(-1))\n",
        "        \n",
        "        # Encode all support instances\n",
        "        flat_support_embeddings = self.sentence_encoder(\n",
        "            flat_input_ids, \n",
        "            attention_mask=flat_attention_mask, \n",
        "            token_type_ids=flat_token_type_ids\n",
        "        )  # (batch_size * support_instance_count, hidden_size)\n",
        "        \n",
        "        # Reshape back\n",
        "        support_embeddings = flat_support_embeddings.view(batch_size, support_instance_count, -1)\n",
        "        \n",
        "        # Apply context attention to highlight important instances\n",
        "        weighted_support = self.context_attention(support_embeddings)\n",
        "        \n",
        "        # Reshape support set for n-way k-shot\n",
        "        support_embeddings = weighted_support.view(batch_size, n_way, k_shot, -1)\n",
        "        \n",
        "        # Calculate prototypes by averaging the k instances for each class\n",
        "        prototypes = support_embeddings.mean(dim=2)  # (batch_size, n_way, hidden_size)\n",
        "        \n",
        "        # Calculate Euclidean distances between query and prototypes\n",
        "        query_expanded = query_embedding.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "        distances = torch.sum((query_expanded - prototypes) ** 2, dim=2)  # (batch_size, n_way)\n",
        "        \n",
        "        # Convert distances to logits (negative distances)\n",
        "        logits = -distances\n",
        "        \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOt9x7x5Cm/ENCEI4+c+LvL",
      "include_colab_link": true,
      "name": "Fine-Tuning BERT for Spam Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
